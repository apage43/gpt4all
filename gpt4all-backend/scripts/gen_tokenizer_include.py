import sys
import tokenizers
import json

def iter_with_last(lst):
    llen = len(lst)
    for i, entry in enumerate(lst):
        last = i == (llen - 1)
        yield last, entry

def separate_trigraphs(ugh):
    while '??' in ugh:
        ugh = ugh.replace("??", "?\"\"?")
    return ugh
    

def main():
    if len(sys.argv) < 3:
        print(f"usage: {sys.argv[0]} <hf tokenizer .json> <name prefix>")
    tkfilename = sys.argv[1]
    prefix = sys.argv[2]
    with open(tkfilename, 'rb') as tkf:
        tokconfig = json.load(tkf)

    print('// @generated GENERATED BY scripts/gen_tokenizer_include.py DO NOT MODIFY')
    print('#include "bpe.h"')
    avilen = len(tokconfig['added_tokens'])
    print(f'const std::array<bpecpp::additional_vocab_item, {avilen}> {prefix}_additional_vocab = ''{{')
    for last, avi in iter_with_last(tokconfig['added_tokens']):
        comma = ',' if not last else '' 
        print('  {'f'.id = {avi["id"]}, .content={json.dumps(separate_trigraphs(avi["content"]))}, .special={json.dumps(avi["special"])}''}' + comma)
    print('}};')
    print()
    vocablen = len(tokconfig['model']['vocab'])
    print(f'constexpr std::array<const char*, {vocablen}> {prefix}_vocab = ''{')
    for last, vi in iter_with_last(tokconfig['model']['vocab']):
        comma = ',' if not last else '' 
        print(f'  {separate_trigraphs(json.dumps(vi))}' + comma)
    print('};')
    print()
    mergeslen = len(tokconfig['model']['merges'])
    print(f'constexpr std::array<const char*, {mergeslen}> {prefix}_merges = ''{')
    for last, mi in iter_with_last(tokconfig['model']['merges']):
        comma = ',' if not last else '' 
        print(f'  {separate_trigraphs(json.dumps(mi))}' + comma)
    print('};')
    

if __name__ == '__main__':
    main()
